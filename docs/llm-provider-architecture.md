# LLM Provider Architecture

> å¯æ’æ‹”çš„ LLM æœåŠ¡å±‚è®¾è®¡ï¼Œæ”¯æŒäº‘ç«¯ APIã€æœ¬åœ°æœåŠ¡ã€ç«¯æ¨¡å‹ï¼Œä»¥åŠæ—  AI å…œåº•

## è®¾è®¡ç›®æ ‡

1. **Provider è§£è€¦** - åº”ç”¨å±‚åªè°ƒç”¨ç»Ÿä¸€æ¥å£ï¼Œä¸æ„ŸçŸ¥å…·ä½“ Provider
2. **å¯æ’æ‹”** - æ–°å¢ Provider åªéœ€å®ç°æ¥å£å¹¶æ³¨å†Œ
3. **è‡ªåŠ¨é™çº§** - Cloud â†’ Local â†’ Edge â†’ Template æ¸è¿›å¼é™çº§
4. **é…ç½®æŒä¹…åŒ–** - ç”¨æˆ· API Key å’Œåå¥½ä¿å­˜åœ¨æœ¬åœ°

## æ¶æ„æ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Application Layer                         â”‚
â”‚  ActivationGenerator / EchoCompanion / OCR                  â”‚
â”‚  (åªè°ƒç”¨ LLMService.chat()ï¼Œä¸å…³å¿ƒå…·ä½“ Provider)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LLMService                               â”‚
â”‚  - ç»Ÿä¸€ API: chat(), generate(), stream()                   â”‚
â”‚  - Provider é€‰æ‹©ä¸å¥åº·æ£€æŸ¥                                    â”‚
â”‚  - è‡ªåŠ¨é™çº§ fallbackChain                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Provider Registry                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ OpenAI   â”‚ â”‚ Anthropicâ”‚ â”‚ Gemini   â”‚ â”‚ WebLLM (Edge)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Ollama   â”‚ â”‚ Template Fallback (æ—  AI å…œåº•)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Provider ç±»å‹

| Type | æè¿° | ç¤ºä¾‹ | ç‰¹ç‚¹ |
|------|------|------|------|
| `cloud` | äº‘ç«¯ API | OpenAI, Anthropic, Gemini | éœ€è¦ API Keyï¼Œè´¨é‡æœ€å¥½ |
| `local` | æœ¬åœ°æœåŠ¡ | Ollama | éœ€è¦æœ¬åœ°è¿è¡ŒæœåŠ¡ |
| `edge` | ç«¯æ¨¡å‹ | WebLLM | æµè§ˆå™¨å†…è¿è¡Œï¼Œé¦–æ¬¡éœ€ä¸‹è½½ |
| `template` | æ¨¡æ¿å…œåº• | - | æ—  AIï¼Œå›ºå®šæ–‡æ¡ˆ |

## æ ¸å¿ƒæ¥å£

### Provider æ¥å£

```typescript
interface LLMProvider {
  readonly id: string;           // 'openai', 'anthropic', 'webllm', etc.
  readonly name: string;         // æ˜¾ç¤ºåç§°
  readonly type: 'cloud' | 'local' | 'edge' | 'template';

  // èƒ½åŠ›å£°æ˜
  capabilities: {
    streaming: boolean;
    maxTokens: number;
  };

  // ç”Ÿå‘½å‘¨æœŸ
  initialize(config: ProviderConfig): Promise<void>;
  isReady(): boolean;
  getStatus(): ProviderStatus;

  // æ ¸å¿ƒæ–¹æ³•
  chat(messages: ChatMessage[], options?: ChatOptions): Promise<string>;
  stream?(messages: ChatMessage[], options?: ChatOptions): AsyncIterable<string>;
}

interface ProviderStatus {
  ready: boolean;
  error?: string;
  // Edge æ¨¡å‹ç‰¹æœ‰
  downloadProgress?: number;  // 0-100
  downloadStatus?: 'idle' | 'downloading' | 'ready' | 'error';
}

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface ChatOptions {
  temperature?: number;
  maxTokens?: number;
  stream?: boolean;
}
```

### é…ç½®æ¥å£

```typescript
interface LLMConfig {
  // ç”¨æˆ·é€‰æ‹©çš„é¦–é€‰ Provider
  activeProvider: string;  // 'openai' | 'anthropic' | 'gemini' | 'ollama' | 'webllm' | 'template'

  // å„ Provider é…ç½®
  providers: {
    openai?: {
      apiKey: string;
      baseUrl?: string;  // æ”¯æŒä»£ç†
      model: string;     // 'gpt-4o-mini', 'gpt-4o', etc.
    };
    anthropic?: {
      apiKey: string;
      model: string;     // 'claude-3-haiku-20240307', etc.
    };
    gemini?: {
      apiKey: string;
      model: string;     // 'gemini-1.5-flash', etc.
    };
    ollama?: {
      baseUrl: string;   // 'http://localhost:11434'
      model: string;     // 'llama3', 'qwen2.5', etc.
    };
    webllm?: {
      model: string;     // 'Qwen2.5-0.5B-Instruct-q4f16_1-MLC'
    };
  };

  // é™çº§ç­–ç•¥: æŒ‰é¡ºåºå°è¯•
  fallbackChain: string[];  // ['openai', 'webllm', 'template']

  // æ˜¯å¦å¯ç”¨è‡ªåŠ¨é™çº§
  autoFallback: boolean;
}
```

## LLMService ä¸»ç±»

```typescript
class LLMService {
  private providers: Map<string, LLMProvider>;
  private config: LLMConfig;

  // æ³¨å†Œ Provider
  registerProvider(provider: LLMProvider): void;

  // è·å–å½“å‰æ´»è·ƒ Provider
  getActiveProvider(): LLMProvider | null;

  // è·å–æ‰€æœ‰å¯ç”¨ Provider
  getAvailableProviders(): LLMProvider[];

  // åˆ‡æ¢ Provider
  setActiveProvider(id: string): Promise<void>;

  // æ›´æ–°é…ç½®
  updateConfig(config: Partial<LLMConfig>): void;

  // æµ‹è¯•è¿æ¥
  testConnection(providerId: string): Promise<{ success: boolean; error?: string }>;

  // æ ¸å¿ƒè°ƒç”¨ - è‡ªåŠ¨å¤„ç†é™çº§
  async chat(messages: ChatMessage[], options?: ChatOptions): Promise<string>;

  // æµå¼è°ƒç”¨
  async *stream(messages: ChatMessage[], options?: ChatOptions): AsyncIterable<string>;
}
```

## ç«¯æ¨¡å‹é€‰æ‹©

### å½“å‰é€‰æ‹©: Qwen2.5-0.5B

| å±æ€§ | å€¼ |
|------|------|
| æ¨¡å‹ | Qwen2.5-0.5B-Instruct |
| å¤§å° | ~500MB (é‡åŒ–å) |
| è¿è¡Œæ—¶ | WebLLM (MLC) |
| è¦æ±‚ | WebGPU æ”¯æŒ |

### åç»­å€™é€‰

| æ¨¡å‹ | å¤§å° | ç‰¹ç‚¹ |
|------|------|------|
| SmolLM2-360M | ~300MB | æ›´å°æ›´å¿« |
| Phi-3-mini | ~2GB | æ¨ç†æ›´å¼º |
| Gemma-2B | ~1.5GB | å¹³è¡¡é€‰æ‹© |

### Mobile ç«¯ (TODO)

> ç ”ç©¶ Apple Intelligence / Core ML åŸç”Ÿèƒ½åŠ›ï¼Œå¯èƒ½ä¸éœ€è¦è‡ªå¸¦æ¨¡å‹

## æ–‡ä»¶ç»“æ„

```
apps/web/src/lib/llm/
â”œâ”€â”€ index.ts                 # å¯¼å‡º LLMService å•ä¾‹
â”œâ”€â”€ types.ts                 # æ‰€æœ‰ç±»å‹å®šä¹‰
â”œâ”€â”€ service.ts               # LLMService å®ç°
â”œâ”€â”€ config.ts                # é…ç½®ç®¡ç† (localStorage)
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ index.ts             # Provider æ³¨å†Œ
â”‚   â”œâ”€â”€ base.ts              # BaseLLMProvider æŠ½è±¡ç±»
â”‚   â”œâ”€â”€ openai.ts            # OpenAI é€‚é…å™¨
â”‚   â”œâ”€â”€ anthropic.ts         # Anthropic é€‚é…å™¨
â”‚   â”œâ”€â”€ gemini.ts            # Gemini é€‚é…å™¨
â”‚   â”œâ”€â”€ ollama.ts            # Ollama é€‚é…å™¨
â”‚   â”œâ”€â”€ webllm.ts            # WebLLM ç«¯æ¨¡å‹
â”‚   â””â”€â”€ template.ts          # æ¨¡æ¿å…œåº•
â””â”€â”€ prompts/
    â”œâ”€â”€ activation.ts        # æ¿€æ´»å¡ç”Ÿæˆ prompt
    â””â”€â”€ echo-companion.ts    # Echo å¯¹è¯ prompt
```

## è®¾ç½®é¡µé¢ UI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Settings > AI Provider                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€ Cloud API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Provider    [OpenAI        â–¼]                       â”‚  â”‚
â”‚  â”‚  API Key     [sk-â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢    ] [Test]     â”‚  â”‚
â”‚  â”‚  Model       [gpt-4o-mini   â–¼]                       â”‚  â”‚
â”‚  â”‚  Base URL    [https://api.openai.com  ] (optional)   â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Status: âœ… Connected                                 â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€ Local Server â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Ollama URL  [http://localhost:11434  ] [Test]       â”‚  â”‚
â”‚  â”‚  Model       [qwen2.5:0.5b  â–¼]                       â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Status: âš ï¸ Not running                               â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€ On-Device Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Model       [Qwen2.5-0.5B  â–¼]                       â”‚  â”‚
â”‚  â”‚  Size        500 MB                                   â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Status: ğŸ“¥ Not downloaded                            â”‚  â”‚
â”‚  â”‚           [Download Model]                            â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 45%                     â”‚  â”‚
â”‚  â”‚  Downloading... 225 MB / 500 MB                      â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€ Active Provider â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Use: (â€¢) Cloud API  ( ) Local  ( ) On-Device        â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  â˜‘ï¸ Auto-fallback when primary unavailable            â”‚  â”‚
â”‚  â”‚     Fallback order: Cloud â†’ Local â†’ On-Device        â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚                                          [Save Settings]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å®ç°è®¡åˆ’

### Phase 1: åŸºç¡€æ¶æ„
- [ ] types.ts - æ¥å£å®šä¹‰
- [ ] config.ts - é…ç½®ç®¡ç†
- [ ] base.ts - Provider åŸºç±»
- [ ] service.ts - LLMService
- [ ] template.ts - æ¨¡æ¿å…œåº• Provider

### Phase 2: Cloud Providers
- [ ] openai.ts
- [ ] anthropic.ts
- [ ] gemini.ts

### Phase 3: è®¾ç½®é¡µé¢
- [ ] Settings è·¯ç”±å’Œé¡µé¢
- [ ] Provider é…ç½® UI
- [ ] è¿æ¥æµ‹è¯•åŠŸèƒ½

### Phase 4: æœ¬åœ°æœåŠ¡
- [ ] ollama.ts

### Phase 5: ç«¯æ¨¡å‹
- [ ] webllm.ts
- [ ] æ¨¡å‹ä¸‹è½½ UI
- [ ] è¿›åº¦æ˜¾ç¤º

### Phase 6: åº”ç”¨é›†æˆ
- [ ] è¿ç§» Activation ç”Ÿæˆ
- [ ] è¿ç§» Echo Session å¯¹è¯
- [ ] ç§»é™¤åç«¯ AI ä¾èµ–

## API å…¼å®¹æ€§è¯´æ˜

### OpenAI æ ¼å¼ (æ ‡å‡†)

```typescript
// OpenAI, Ollama, å¤§å¤šæ•°å…¼å®¹æœåŠ¡
{
  model: "gpt-4o-mini",
  messages: [
    { role: "system", content: "..." },
    { role: "user", content: "..." }
  ]
}
```

### Anthropic æ ¼å¼

```typescript
// Anthropic ç‰¹æ®Š: system å•ç‹¬ä¼ 
{
  model: "claude-3-haiku-20240307",
  system: "...",
  messages: [
    { role: "user", content: "..." }
  ]
}
```

### Gemini æ ¼å¼

```typescript
// Gemini: ä½¿ç”¨ contents è€Œé messages
{
  contents: [
    { role: "user", parts: [{ text: "..." }] }
  ],
  systemInstruction: { parts: [{ text: "..." }] }
}
```

å„ Provider é€‚é…å™¨è´Ÿè´£æ ¼å¼è½¬æ¢ï¼Œå¯¹å¤–ç»Ÿä¸€ä½¿ç”¨ OpenAI æ ¼å¼ã€‚
